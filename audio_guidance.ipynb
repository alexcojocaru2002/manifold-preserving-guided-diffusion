{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e637a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import wav2clip\n",
    "from typing import Dict\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5e991c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "\n",
    "# 1) Loading a tensor or generic Python object\n",
    "pretrained_weights = torch.load(\"Wav2CLIP.pt\")\n",
    "\n",
    "# image_transform_obj = {k: v for k, v in obj.items() if \"image_transform\" in k}\n",
    "# print(obj['audio_transform.sequential.3.weight'])\n",
    "# pprint([el for el in image_transform_obj.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818f005",
   "metadata": {},
   "source": [
    "# Load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a4c4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\Konstantinos\\AppData\\Local\\Temp\\ipykernel_10128\\1928589449.py:1: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  image_path = \"images\\cat.jpg\"\n"
     ]
    }
   ],
   "source": [
    "image_path = \"images\\cat.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image = image.resize((512, 512))\n",
    "# Preprocess: convert to tensor normalized in [-1, 1]\n",
    "image_tensor = (\n",
    "    transforms.ToTensor()(image).unsqueeze(0).to(device)\n",
    ")  # shape [1,3,H,W]\n",
    "image_tensor = 2.0 * image_tensor - 1.0  # scale from [0,1] to [-1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8620efef",
   "metadata": {},
   "source": [
    "# Get CLIP embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc899711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessing_for_clip():\n",
    "    return transforms.Compose([\n",
    "        # 1) Resize shorter edge to target_size, keep aspect ratio:\n",
    "        transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "        \n",
    "        # 2) Center‐crop to exactly (target_size, target_size):\n",
    "        transforms.CenterCrop(224),\n",
    "        \n",
    "        # 3) Normalize per‐channel:\n",
    "        transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c497ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model\n",
    "clip_model_vit, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model_res, _ = clip.load(\"RN50\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf9258f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_preprocessing = get_preprocessing_for_clip()\n",
    "preprocessed_image_tensor = clip_preprocessing(image_tensor)\n",
    "preprocessed_image_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0587e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding_vit = clip_model_vit.encode_image(preprocessed_image_tensor)\n",
    "image_embedding_vit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d620d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding_res = clip_model_res.encode_image(preprocessed_image_tensor)\n",
    "image_embedding_res.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4384112",
   "metadata": {},
   "source": [
    "# Wav2CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5b2848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = \"audio/1-100032-A-0.wav\"\n",
    "import librosa\n",
    "\n",
    "wav2clip_model = wav2clip.get_model()\n",
    "wav, sr = librosa.load(audio_path, sr=48000, mono=True)\n",
    "audio_tensor = wav\n",
    "audio_emb_batch = wav2clip.embed_audio(audio_tensor, wav2clip_model)\n",
    "audio_emb_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6bffbe",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50bc6ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0768], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.nn.CosineSimilarity(dim=-1)(image_embedding_vit, torch.from_numpy(audio_emb_batch).to(device)))\n",
    "# print(torch.nn.CosineSimilarity(dim=-1)(image_embedding_res, torch.from_numpy(audio_emb_batch).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d74ada31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPLayers(nn.Module):\n",
    "    def __init__(self, units=[512, 512, 512], nonlin=nn.ReLU(), dropout=0.1):\n",
    "        super(MLPLayers, self).__init__()\n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = dropout\n",
    "\n",
    "        sequence = []\n",
    "        for u0, u1 in zip(units[:-1], units[1:]):\n",
    "            sequence.append(nn.Linear(u0, u1))\n",
    "            sequence.append(self.nonlin)\n",
    "            sequence.append(nn.Dropout(self.dropout))\n",
    "        sequence = sequence[:-2]\n",
    "\n",
    "        self.sequential = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.sequential(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f09c47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_strip_state_dict(\n",
    "    state_dict: Dict[str, torch.Tensor],\n",
    "    prefix: str\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Keep only items where key starts with prefix, then strip the prefix.\n",
    "    Example:\n",
    "      'image_transform.sequential.0.weight' -> 'sequential.0.weight'\n",
    "    \"\"\"\n",
    "    filtered = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if key.startswith(prefix + '.'):\n",
    "            new_key = key[len(prefix) + 1:]\n",
    "            filtered[new_key] = value\n",
    "    return filtered\n",
    "\n",
    "def load_mlp_weights(\n",
    "    mlp: nn.Module,\n",
    "    state_dict: Dict[str, torch.Tensor],\n",
    "    prefix: str\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load weights from a flat state_dict into the MLP.\n",
    "\n",
    "    Args:\n",
    "      mlp: Instance of MLPLayers\n",
    "      state_dict: Dict mapping prefixed keys to tensors\n",
    "      prefix: Prefix to filter keys (e.g., 'image_transform')\n",
    "    \"\"\"\n",
    "    # Extract relevant weights\n",
    "    mlp_weights = filter_and_strip_state_dict(state_dict, prefix)\n",
    "\n",
    "    # Load into model's state dict\n",
    "    model_dict = mlp.state_dict()\n",
    "    model_dict.update(mlp_weights)\n",
    "    mlp.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71515b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPLayers(\n",
       "  (nonlin): ReLU()\n",
       "  (sequential): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate MLP and load weights\n",
    "image_transform = MLPLayers(units=[512, 512, 512]).to(device)\n",
    "load_mlp_weights(image_transform, pretrained_weights, prefix='image_transform')\n",
    "image_transform.eval()\n",
    "# # Verify loaded state\n",
    "# for name, param in image_transform.named_parameters():\n",
    "#     print(f\"{name}: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b8d83a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPLayers(\n",
       "  (nonlin): ReLU()\n",
       "  (sequential): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate MLP and load weights\n",
    "audio_transform = MLPLayers(units=[512, 512, 512]).to(device)\n",
    "load_mlp_weights(audio_transform, pretrained_weights, prefix='audio_transform')\n",
    "audio_transform.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c4418b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding = image_embedding_vit.float()\n",
    "audio_embedding = torch.from_numpy(audio_emb_batch).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd12732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_image = image_transform(image_embedding)\n",
    "transformed_audio = audio_transform(audio_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e162717d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0768], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([-0.0967], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "tensor([-0.0080], device='cuda:0', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "def compute_loss(image, audio):\n",
    "    logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "    \n",
    "\n",
    "print(torch.nn.CosineSimilarity(dim=-1)(image_embedding, audio_embedding))\n",
    "print(torch.nn.CosineSimilarity(dim=-1)(transformed_image, audio_embedding))\n",
    "print(torch.nn.CosineSimilarity(dim=-1)(transformed_image, transformed_audio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f30f4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPLoss1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPLoss1D, self).__init__()\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.loss_image = nn.CrossEntropyLoss()\n",
    "        self.loss_text = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, image_features, text_features):\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.t()\n",
    "        logits_per_text = logit_scale * text_features @ image_features.t()\n",
    "\n",
    "        batch_size = image_features.shape[0]\n",
    "        ground_truth = torch.arange(batch_size, dtype=torch.long, device=device)\n",
    "        return (\n",
    "            self.loss_image(logits_per_image, ground_truth)\n",
    "            + self.loss_text(logits_per_text, ground_truth)\n",
    "        ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d298525d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = CLIPLoss1D()\n",
    "loss(image_embedding, audio_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82313fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9051f58",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniforge3\\envs\\gm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniforge3\\envs\\gm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mCLIPLoss1D.forward\u001b[39m\u001b[34m(self, image_features, text_features)\u001b[39m\n\u001b[32m     17\u001b[39m batch_size = image_features.shape[\u001b[32m0\u001b[39m]\n\u001b[32m     18\u001b[39m ground_truth = torch.arange(batch_size, dtype=torch.long, device=device)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits_per_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m     + \u001b[38;5;28mself\u001b[39m.loss_text(logits_per_text, ground_truth)\n\u001b[32m     22\u001b[39m ) / \u001b[32m2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniforge3\\envs\\gm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniforge3\\envs\\gm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniforge3\\envs\\gm\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1297\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniforge3\\envs\\gm\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "loss(torch.ones(size=[4, 512]), torch.ones(size=[4, 512]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
